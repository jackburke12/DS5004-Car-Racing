# -*- coding: utf-8 -*-
"""duelingdqn_carracing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxXRKUMeqHi-13gvwQT3-nqX52d5VrR4
"""

"""
Dueling Double DQN for CarRacing-v3 (Gymnasium)
Discrete 5-action mapping:
0: do nothing
1: steer right
2: steer left
3: gas
4: brake

PyTorch implementation.
"""
import os
import random
import time
from collections import deque
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gymnasium as gym

# -------------------------
# Hyperparameters
# -------------------------
ENV_ID = "CarRacing-v3"
SEED = 42

IMG_H = 84
IMG_W = 84
NUM_STACK = 3

NUM_ACTIONS = 5

REPLAY_BUFFER_SIZE = 150000
BATCH_SIZE = 32
GAMMA = 0.99
LR = 1e-4
TARGET_UPDATE_EVERY = 500  # environment steps
WARMUP_STEPS = 5000
TRAIN_EVERY = 1
UPDATES_PER_STEP = 1

EPS_START = 1.0
EPS_END = 0.05
EPS_DECAY_STEPS = 500000

MAX_EPISODES = 1500
MAX_STEPS_PER_EPISODE = 1000

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -------------------------
# Discrete actions mapping
# -------------------------
ACTIONS = [
    np.array([0.0, 0.0, 0.0], dtype=np.float32),   # 0: do nothing
    np.array([+1.0, 0.0, 0.0], dtype=np.float32),  # 1: steer right
    np.array([-1.0, 0.0, 0.0], dtype=np.float32),  # 2: steer left
    np.array([0.0, 1.0, 0.0], dtype=np.float32),   # 3: gas
    np.array([0.0, 0.0, 0.8], dtype=np.float32),   # 4: brake
]

# -------------------------
# Preprocessing
# -------------------------
def preprocess_frame(frame):
    """RGB -> grayscale, resize, normalize to [0,1]"""
    img = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    img = cv2.resize(img, (IMG_W, IMG_H), interpolation=cv2.INTER_AREA)
    return img.astype(np.float32) / 255.0  # shape (H, W)

# -------------------------
# Frame stack helper
# -------------------------
class FrameStack:
    def __init__(self, k):
        self.k = k
        self.deque = deque(maxlen=k)

    def reset(self, frame):
        p = preprocess_frame(frame)
        for _ in range(self.k):
            self.deque.append(p)
        return np.stack(self.deque, axis=0).astype(np.float32)  # (C, H, W)

    def append(self, frame):
        p = preprocess_frame(frame)
        self.deque.append(p)
        return np.stack(self.deque, axis=0).astype(np.float32)

# -------------------------
# Replay buffer
# -------------------------
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        # state: (C,H,W) numpy
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))
        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)

# -------------------------
# Dueling Q-network
# -------------------------
class DuelingQNetwork(nn.Module):
    def __init__(self, num_actions=NUM_ACTIONS, in_channels=NUM_STACK):
        super().__init__()
        # conv body (standard DQN-style)
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
        )
        # compute flattened size
        with torch.no_grad():
            dummy = torch.zeros(1, in_channels, IMG_H, IMG_W)
            n_flatten = self.conv(dummy).view(1, -1).shape[1]

        # advantage stream
        self.adv_fc = nn.Sequential(
            nn.Linear(n_flatten, 512),
            nn.ReLU(),
            nn.Linear(512, num_actions)
        )
        # value stream
        self.val_fc = nn.Sequential(
            nn.Linear(n_flatten, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

    def forward(self, x):
        # x: (B, C, H, W)
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        adv = self.adv_fc(x)   # (B, A)
        val = self.val_fc(x)   # (B, 1)
        # combine: Q = V + (A - mean(A))
        q = val + (adv - adv.mean(dim=1, keepdim=True))
        return q  # (B, A)

# -------------------------
# Agent: Dueling Double DQN
# -------------------------
class Agent:
    def __init__(self):
        self.online = DuelingQNetwork().to(DEVICE)
        self.target = DuelingQNetwork().to(DEVICE)
        self.target.load_state_dict(self.online.state_dict())
        self.optimizer = optim.Adam(self.online.parameters(), lr=LR)
        self.replay = ReplayBuffer(REPLAY_BUFFER_SIZE)
        self.total_steps = 0
        self.eps = EPS_START

    def select_action(self, state, eval_mode=False):
        """state: (C,H,W) numpy"""
        if eval_mode:
            eps = 0.01
        else:
            eps = max(EPS_END, EPS_START - (EPS_START - EPS_END) * (self.total_steps / EPS_DECAY_STEPS))
        self.eps = eps

        if (not eval_mode) and (random.random() < eps):
            return random.randrange(NUM_ACTIONS)
        st = torch.tensor(state[None, :], dtype=torch.float32, device=DEVICE)  # (1,C,H,W)
        with torch.no_grad():
            qvals = self.online(st)  # (1,A)
        return int(qvals.argmax(dim=1).item())

    def store(self, *args):
        self.replay.push(*args)

    def hard_update_target(self):
        self.target.load_state_dict(self.online.state_dict())

    def update(self):
        if len(self.replay) < BATCH_SIZE:
            return None

        states, actions, rewards, next_states, dones = self.replay.sample(BATCH_SIZE)

        # to tensors
        states_t = torch.tensor(states, dtype=torch.float32, device=DEVICE)        # (B,C,H,W)
        next_states_t = torch.tensor(next_states, dtype=torch.float32, device=DEVICE)
        actions_t = torch.tensor(actions, dtype=torch.long, device=DEVICE)        # (B,)
        rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)
        dones_t = torch.tensor(dones.astype(np.uint8), dtype=torch.float32, device=DEVICE)

        # current Q(s,a) from online
        q_values = self.online(states_t)                      # (B,A)
        q_s_a = q_values.gather(1, actions_t.unsqueeze(1)).squeeze(1)  # (B,)

        # Double DQN target:
        # online picks argmax next action; target evaluates Q_target(next_state, argmax)
        with torch.no_grad():
            next_q_online = self.online(next_states_t)               # (B,A)
            next_actions = next_q_online.argmax(dim=1, keepdim=True) # (B,1)
            next_q_target = self.target(next_states_t)               # (B,A)
            next_q_value = next_q_target.gather(1, next_actions).squeeze(1)  # (B,)
            target = rewards_t + (1.0 - dones_t) * GAMMA * next_q_value

        loss = F.smooth_l1_loss(q_s_a, target)

        self.optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(self.online.parameters(), 10.0)
        self.optimizer.step()

        return loss.item()

# -------------------------
# Training loop
# -------------------------
def train():
    name = "dueling_double_dqn"
    env = gym.make(ENV_ID, render_mode=None)
    env.reset(seed=SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)

    agent = Agent()
    fs = FrameStack(NUM_STACK)

    global_step = 0
    episode_rewards = []
    avg50_rewards = []

    print(f"\nStarting training the {name} agent...")
    checkpoint_dir = f"./{name}_carracing_checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)

    for ep in range(1, MAX_EPISODES + 1):
        obs, info = env.reset()
        state = fs.reset(obs)
        ep_reward = 0.0

        for step in range(MAX_STEPS_PER_EPISODE):
            action_idx = agent.select_action(state)
            cont_action = ACTIONS[action_idx]

            next_obs, reward, terminated, truncated, info = env.step(cont_action)
            done = terminated or truncated
            next_state = fs.append(next_obs)

            agent.store(state, action_idx, reward, next_state, done)

            state = next_state
            ep_reward += reward
            global_step += 1
            agent.total_steps = global_step

            # training step(s)
            if global_step > WARMUP_STEPS and global_step % TRAIN_EVERY == 0:
                for _ in range(UPDATES_PER_STEP):
                    agent.update()

            # hard update
            if global_step % TARGET_UPDATE_EVERY == 0:
                agent.hard_update_target()

            if done:
                break

        episode_rewards.append(ep_reward)
        avg50 = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 1 else ep_reward
        avg50_rewards.append(avg50)
        print(f"Episode: {ep:4d} | EpisodeReward: {ep_reward:7.1f} | Avg50EpReward: {avg50:7.2f} | Epsilon: {agent.eps:.3f}")

        if ep % 50 == 0:
            torch.save(agent.online.state_dict(), f"./{checkpoint_dir}/{name}_carracing_v3_ep{ep}.pth")
            print(f"{name} checkpoint saved at episode {ep}")

    training_results = {"EpisodeReward": episode_rewards, "Avg50EpReward": avg50_rewards}
    results_df = pd.DataFrame(training_results)
    results_df.to_csv(f'{name}_training_results.csv', index=False)
    print(f"\nTraining the {name} agent finished! Training_results saved to '{name}_training_results.csv'")
    
    env.close()
    return agent

# -------------------------
# Evaluation / play
# -------------------------
def evaluate(agent, episodes=3, render=True):
    name = "dueling_double_dqn"
    env = gym.make(ENV_ID, render_mode="human" if render else None)
    fs = FrameStack(NUM_STACK)

    for ep in range(1, episodes + 1):
        obs, info = env.reset()
        state = fs.reset(obs)
        total_reward = 0.0
        done = False
        steps = 0
        while not done and steps < MAX_STEPS_PER_EPISODE:
            action_idx = agent.select_action(state, eval_mode=True)
            cont_action = ACTIONS[action_idx]
            obs, reward, terminated, truncated, info = env.step(cont_action)
            done = terminated or truncated
            state = fs.append(obs)
            total_reward += reward
            steps += 1
            if render:
                time.sleep(0.01)
        print(f"Evaluation of trained {name} agent - Episode {ep} reward: {total_reward:.1f}")
    env.close()

# -------------------------
# Run
# -------------------------
if __name__ == "__main__":
    agent = train()
    print("Training complete â€” running a short evaluation with rendering.")
    evaluate(agent, episodes=3, render=True)